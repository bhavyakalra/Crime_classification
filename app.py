# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mcNOrPWf2G9QBBnzzN4P1PEQkA1SF-jT
"""

!pip install streamlit
!pip install pyspark
import streamlit as st
from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, HashingTF, IDF, Word2Vec
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import lower,col

# Set up Spark Session
spark = SparkSession.builder.appName("CrimeClassification").getOrCreate()

# Define functions to load and preprocess data
def load_data():
    data = spark.read.csv("/content/train.csv", header=True, inferSchema=True)
    return data

def preprocess_data(data):
    data = data.select(lower(col('Category')).alias('Category'), lower(col('Descript')).alias('Descript'))
    return data


# Define functions to create and evaluate models
def create_model(data, model_type):
    # Preprocess data
    data = preprocess_data(data)

    # Define stages for the pipeline
    regex_tokenizer = RegexTokenizer(inputCol="Description", outputCol="tokens", pattern="\\W")
    stopwords_remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_words", stopWords=['http','amp','rt','t','c','the'])
    count_vectors = CountVectorizer(inputCol="filtered_words", outputCol="features", vocabSize=10000, minDF=5)
    label_string_idx = StringIndexer(inputCol="Category", outputCol="label")

    # Create the pipeline
    pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, count_vectors, label_string_idx, model_type])

    # Fit the model
    model = pipeline.fit(data)
    return model

def evaluate_model(model, test_data):
    predictions = model.transform(test_data)
    evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    return accuracy

# Load the data
data = load_data()

# Split data into training and test sets
training_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

# Define models
lr_model = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)
nb_model = NaiveBayes(smoothing=1)
dt_model = DecisionTreeClassifier()
rf_model = RandomForestClassifier()

# Train models
lr_trained_model = create_model(training_data, lr_model)
nb_trained_model = create_model(training_data, nb_model)
dt_trained_model = create_model(training_data, dt_model)
rf_trained_model = create_model(training_data, rf_model)

# Evaluate models
lr_accuracy = evaluate_model(lr_trained_model, test_data)
nb_accuracy = evaluate_model(nb_trained_model, test_data)
dt_accuracy = evaluate_model(dt_trained_model, test_data)
rf_accuracy = evaluate_model(rf_trained_model, test_data)

# Streamlit app
st.title("Crime Classification Models")
st.write("Accuracy of different models:")
st.write("- Logistic Regression:", lr_accuracy)
st.write("- Naive Bayes:", nb_accuracy)
st.write("- Decision Tree Classifier:", dt_accuracy)
st.write("- Random Forest Classifier:", rf_accuracy)

!pip install streamlit
!pip install pyspark
import streamlit as st
from pyspark.ml.classification import LogisticRegression, NaiveBayes, DecisionTreeClassifier, RandomForestClassifier
from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, StringIndexer, HashingTF, IDF, Word2Vec
from pyspark.ml import Pipeline
from pyspark.sql import SparkSession
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.linalg import Vectors
from pyspark.sql.functions import lower,col

spark = SparkSession.builder.appName("CrimeClassification").getOrCreate()

# Define functions to load and preprocess data
def load_data():
    data = spark.read.csv("/content/train.csv", header=True, inferSchema=True)
    return data

def preprocess_data(data):
    # Check if the 'Description' column exists
    if 'Description' in data.columns:
        data = data.select(lower(col('Category')).alias('Category'), lower(col('Description')).alias('Descript'))
    else:
        raise ValueError("The 'Description' column does not exist in the dataset.")
    return data

def create_model(data, model_type):
    # Preprocess data
    training_data = preprocess_data(data)

    # Define stages for the pipeline
    regex_tokenizer = RegexTokenizer(inputCol="Description", outputCol="tokens", pattern="\\W")
    stopwords_remover = StopWordsRemover(inputCol="tokens", outputCol="filtered_words", stopWords=['http','amp','rt','t','c','the'])
    count_vectors = CountVectorizer(inputCol="filtered_words", outputCol="features", vocabSize=10000, minDF=5)
    label_string_idx = StringIndexer(inputCol="Category", outputCol="label")

    # Create the pipeline
    pipeline = Pipeline(stages=[regex_tokenizer, stopwords_remover, count_vectors, label_string_idx, model_type])

    # Fit the model
    model = pipeline.fit(data)
    return model

def evaluate_model(model, test_data):
    predictions = model.transform(test_data)
    evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
    accuracy = evaluator.evaluate(predictions)
    return accuracy

data = load_data()

# Split data into training and test sets
training_data, test_data = data.randomSplit([0.8, 0.2], seed=42)

# Define models
lr_model = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)
nb_model = NaiveBayes(smoothing=1)
dt_model = DecisionTreeClassifier()
rf_model = RandomForestClassifier()

lr_trained_model = create_model(training_data, lr_model)
nb_trained_model = create_model(training_data, nb_model)
dt_trained_model = create_model(training_data, dt_model)
rf_trained_model = create_model(training_data, rf_model)

